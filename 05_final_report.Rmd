---
title: "MSD 2019 Final Project"
subtitle: "A replication and extension of Comparing Random Forest with Logistic Regression for Predicting Class-Imbalanced Civil War Onset Data by David Muchlinski, David Siroky, et. al., October 22, 2015"
author: "Your Names (your unis)"
date: '`r Sys.time()`'
output:
  pdf_document:
    toc: yes
    toc_depth: 3
---

```{r setup, include=FALSE}
# Setting up dependencies
library(randomForest)
library(caret)
library(ROCR)
library(pROC)
library(stepPlr)
library(doMC)
library(tidyverse)
library(foreign) # for reading Stata .dta files
library(maptools) # for displaying countries on world map
library(DMwR) # for up-sampling with SMOTE

theme_set(theme_bw())

knitr::opts_chunk$set(echo = TRUE)

registerDoMC(cores=7) ## distributing workload over multiple cores for faster computaiton
```

# Introduction

## Motivation
Prediction is at the heart of many machine learning and data science applications, and its importance is amplified in the context of political science. Especially for the case of civil war onset, robust models that can make correct predictions has the potential to save millions of lives and guide political agendas for the years to come.

## Paper Description
Comparing Random Forest with Logistic Regression for Predicting Class-Imbalanced Civil War Onset Data by David Muchlinski, David Siroky, et. al. is an explaratory research paper that, among several other things, makes the argument that most well-known logistic models acquire relatively lower predictive powers for civil war onsets, and shows that a custom random forest model achieves much higer prediction accuricies.

## Replication Description
The replication for the paper mentioned will consist of two main parts, which will go parallel to each other as represented in this R Notebook. The first part will deal with extracting code snippets from the original R code (found in ```original_code/```), and questioning the reasonability and correctness of the methods and code semantics used, as well as an effort to see if results can be completely replicated. The second part will deal with suggesting improvements, modifications, and eventually corrections to the original R code, where we will try to report summaries of each model using fair metrics and correct methodologies. 

Later, a third part will deal with suggesting new models...

# Data Exploration
```{r data-reading-and-exploration}
# HS_original: Civil War Data by Hegre and Sambanis (2006), the original version
data_HS_original <- read.dta(file="data/Sambanis (Aug 06).dta")

# HS_cleaned: Civil War Data by Hegre and Sambanis (2006), NAs eliminated version
data_HS_cleaned <- na.omit(data_HS_original)

# HS_imputed: Civil War Data by Hegre and Sambanis (2006), imputed by authors
data_HS_imputed <- read.csv(file="data/SambnisImp.csv") ## data for prediction

# AM_imputed: Amelia dataset imputed by authors
data_AM_imputed <- read.csv(file="data/Amelia.Imp3.csv") ## data for causal machanisms

# AF_imputed: Africa dataset imputed by authors
data_AF_imputed <- read.csv(file="data/AfricaImp.csv")

data_presentation <- matrix(c(ncol(data_HS_original), sum(is.na(data_HS_original)),
                              nrow(data_HS_original),
                              ncol(data_HS_cleaned), sum(is.na(data_HS_cleaned)),
                              nrow(data_HS_cleaned),
                              ncol(data_HS_imputed), sum(is.na(data_HS_imputed)),
                              nrow(data_HS_imputed),
                              ncol(data_AM_imputed), sum(is.na(data_AM_imputed)),
                              nrow(data_AM_imputed),
                              ncol(data_AF_imputed), sum(is.na(data_AF_imputed)),
                              nrow(data_AF_imputed)), ncol=5)
colnames(data_presentation) <- c('HS_original', 'HS_cleaned', 
                                 'HS_imputed', 'AM_imputed', 'AF_imputed')
rownames(data_presentation) <- c('No. features', 'No. empty cells', 'No. examples')
as.data.frame(data_presentation)

# Check intersection and difference of features on two datasets
# setdiff(colnames(data_HS_imputed), colnames(data_HS_original))
# length(intersect(colnames(data_HS_imputed), colnames(data_HS_original)))
# intersect(colnames(data_HS_imputed), colnames(data_AF_imputed))

# Function for converting a specified dependent variable into factor levels
Y_factor <- function(dataset_column) {
  return(factor(dataset_column, levels=c(0,1), labels=c("peace", "war")))
}

# Convert 'warstds' column into 'peace' or 'war' (initially marked 0 or 1)
data_HS_original$warstds <- Y_factor(data_HS_original$warstds)
data_HS_imputed$warstds <- Y_factor(data_HS_imputed$warstds)
data_AM_imputed$warstds <- Y_factor(data_AM_imputed$warstds)
data_AF_imputed$warstds <- Y_factor(data_AF_imputed$warstds)

# Visualize countries with civil war for each dataset
data(wrld_simpl)
HS_original_countries <- wrld_simpl@data$NAME %in% 
  data_HS_original[data_HS_original$warstds=='war',]$country
AM_imputed_countries <- wrld_simpl@data$NAME %in% 
  data_AM_imputed[data_AM_imputed$warstds=='war',]$country

plot(wrld_simpl, col=c(gray(.80),"red")[HS_original_countries+1], 
     main='Original Hegre and Sambanis (2006)')
plot(wrld_simpl, col=c(gray(.80),"red")[AM_imputed_countries+1],
     main='Imputed Amelia Dataset')
```

There are a few questions that remained unanswered after replicating the data reading and processing done by authors. 

The reported presentation data frame above shows that the original Hegre and Sambanis (2006) dataset, denoted by ```HS_original```, has 9691 examples, 284 features, and 979981 cells containing missing (NA) values. The dataset that is constructed by ourselves when these cells were omitted, ```HS_cleaned```, shows that every single row of the original dataset contains some missing values. Obviously, this dataset is dropped from now on as it doesn't contain any entries. ```HS_imputed```, the dataset imputed by authors on the other hand, has 7140 examples, 286 features, and 0 cells containing missing (NA) values. It is unclear and unmentioned how and why the authors have imputed this dataset, filled all cells with missing values, and deleted ~2500 examples from the original dataset. The second dataset imputed by authors, denoted ```AM_imputed```, has 7141 examples, 53 features, and 778 cells containing missing (NA) values. Lastly, the third dataset imputed by authors, denoted ```AF_imputed```, has 737 examples, 11 features, and 0 cells containing missing (NA) values. This dataset is also dropped, because i) no country information exists whatsoever on examples, and more importantly ii) none of the features (columns) in this dataset match with any of the other features in the other datasets (except the dependent variable). 

It is also unclear why the paper needs two different imputed datasets. The ```AM_imputed``` dataset is supposed to be the smaller dataset where features theorized to be most relevant to the onset of civil war are imputed. Although the number of features decrease as claimed, 778 cells with missing (NA) values reappear in this dataset, and the number of examples increase by 1 without any explanation. For comparison and metrics reporting purposes anyway, it is usually a better idea to select a singular dataset and train & test models on this same dataset through a reasonable splitting of data. On the contrary, the authors have only used in-sample metric to report the accuracies of the models, hence the reported accuracies don't really say much. In order to assess performance, we have to test our models for out-of-sample data.

```{r class-imbalance-exploration}
# Explore class imbalance
table(data_HS_original$warstds)
table(data_HS_imputed$warstds)
table(data_AM_imputed$warstds)
table(data_AF_imputed$warstds)
```

With the first three reported tables, it seems that authors have either deleted or modified examples from the ```HS_original``` dataset where the ```warstds``` variable was NA when they were preparing their imputed dataset ```HS_imputed```. Hence, they increased examples of peace by 800 examples in their imputation. In such a class-imbalanced data (as displayed by tables above), one would imagine i) down-sampling from the majority class or ii) up-sampling from the minority class would be the reasonable action, rather than increasing the number of examples of the majority class. Hence, we will implement these two techniques, i) and ii), and see if they yield better results.

SMOTE (Synthetic Minority Over-sampling Technique) is designed for problems when one class dominates the other, which usually happens in rare-event occurrences. We can easily make the argument that it is thus appropraite for the civil war onset data at hand. The general idea of this method is to artificially generate new examples of the minority class using the nearest neighbors of these cases. Furthermore, the majority class examples are also under-sampled, leading to a more balanced dataset. (RDocumentation) Hence, it is a combination of i) and ii).

# Data Processing, SMOTE, and Training/Test Split for Accurate Assessment
Based on our exploration, we will drop every dataset discussed up until now except ```HS_imputed```, and do a split on this dataset. This is to prevent any ambiguity, and also to respect the research done by Muchlinski et. al. (2016). Although it is unmentioned how they removed examples and filled missing values, this dataset seems the most workable with when the other datasets are considered. The specific splitting strategy is the one that was mentioned but not implemented by the authors: Examples spanning years 1945-1990 will construct the training set, whereas examples after 1990 will construct the testing set.

```{r data-processing-and-split}
# Perform the split
split_year <- 1990
data_train <- data_HS_imputed[data_HS_imputed$year <= split_year, ]
data_test <- data_HS_imputed[data_HS_imputed$year > split_year, ]

# Observe class-imbalance in training and testing sets
table(data_train$warstds)
table(data_test$warstds)

# Since we are done with visualizations and exploration, refactor the dependent variable
data_train$warstds <- as.factor(data_train$warstds)
data_test$warstds <- as.factor(data_test$warstds)

# SMOTE for artificially creating a more balanced training dataset
data_train_balanced <- SMOTE(warstds ~ ., data_train, perc.over = 600, perc.under=100)
table(data_train_balanced$warstds)
```

# Model Specifications
To keep track of model specifications which use different numbers and types of features based on either theory or computations, we propose that we explicitly define features and hence accompanying formulas to be used by models later in this next code block. The 4 specifications the paper have covered are:

1) Fearon and Laitin specification (2003) consisting of 11 variables
2) Collier and Hoeffler specification (2004) consisting of 12 variables
3) Hegre and Sambanis specification (2006) consisting of 20 variables
4) The authors' specifications consisting of 88 variables, selected from Sambanis (2006) index

```{r data-feature-selection}
# Function to generate a formula given dependent variable label and features
# Ex: get_model_formula('height', c('age', 'weight')) : height ~ age + weight
get_model_formula <- function(label, feature_vector) {
  formula_string <- ""
  for (feature in feature_vector) {
    formula_string <- paste(formula_string, feature, "+")
  }
  formula_string <- substring(formula_string, 1, nchar(formula_string)-1)
  return(as.formula(paste(paste(label, "~"), formula_string)))
}

# Specify the dependent variable that will be predicted in all models
y_var <- "warstds"

# The 88 variables selected by authors from Sambanis (2006) Appendix as spec of their RF model
author_vars <- c("ager", "agexp", "anoc", "army85", "autch98", "auto4",
                 "autonomy", "avgnabo", "centpol3", "coldwar", "decade1", "decade2",
                 "decade3", "decade4", "dem", "dem4", "demch98", "dlang", "drel",
                 "durable", "ef", "ef2", "ehet", "elfo", "elfo2", "etdo4590",
                 "expgdp", "exrec", "fedpol3", "fuelexp", "gdpgrowth", "geo1", "geo2",
                 "geo34", "geo57", "geo69", "geo8", "illiteracy", "incumb", "infant",
                 "inst", "inst3", "life", "lmtnest", "ln_gdpen", "lpopns", "major", "manuexp",
                 "milper", "mirps0", "mirps1", "mirps2", "mirps3", "nat_war", "ncontig",
                 "nmgdp", "nmdp4_alt", "numlang", "nwstate", "oil", "p4mchg",
                 "parcomp", "parreg", "part", "partfree", "plural", "plurrel",
                 "pol4", "pol4m", "pol4sq", "polch98", "polcomp", "popdense",
                 "presi", "pri", "proxregc", "ptime", "reg", "regd4_alt", "relfrac", 
                 "seceduc", "second", "semipol3", "sip2", "sxpnew", "sxpsq", "tnatwar",
                 "trade", "warhist", "xconst")
author_spec <- get_model_formula(y_var, author_vars)

# The 11 variables selected by Fearon and Laitin (2003) as spec of their LR model
FL_vars <- c("warhist", "ln_gdpen", "lpopns", "lmtnest", "ncontig",
             "oil", "nwstate", "inst3", "pol4", "ef", "relfrac")
FL_spec <- get_model_formula(y_var, FL_vars)

# The 12 variables selected by Collier and Hoeffler (2004) as spec of their LR model
CH_vars <- c("sxpnew", "sxpsq", "ln_gdpen", "gdpgrowth", "warhist", "lmtnest", 
             "ef", "popdense", "lpopns", "coldwar", "seceduc", "ptime")
CH_spec <- get_model_formula(y_var, CH_vars)

# The 20 variables selected by Hegre and Sambanis (2006) as spec of their LR model
HS_vars <- c("lpopns", "ln_gdpen", "inst3", "parreg", "geo34", "proxregc", "gdpgrowth",
             "anoc", "partfree", "nat_war", "lmtnest", "decade1", "pol4sq", "nwstate", 
             "regd4_alt", "etdo4590", "milper", "geo1", "tnatwar", "presi")
HS_spec <- get_model_formula(y_var, HS_vars)
```

Below is the setup to be run before training. We are not changing anything here. One thing to look for is that we will set the seed for each caret training function call, so that all of the results presented here are replicable. The original code by the authors only set the seed once, which unfortunately makes their results unreplicable.

```{r setup-before-training}
set.seed(666) ## the most metal seed for CV

# Specify number of folds (10 by default, suggested by authors)
num_folds <- 10
cv_index <- createFolds(factor(data_train$warstds), num_folds, returnTrain=T)
## This method of data slicing - or CV - will be used for all logit models
tc <- trainControl(method="cv", 
                   index=cv_index,
                   number=num_folds, ## creates CV folds - 10 for this data
                   summaryFunction=twoClassSummary, ## provides ROC stats in call to model
                   classProb=T)
```

Now that everything is set up, we will train and test logistic regression and random forest models (* only in-sample as of now) using the 4 specifications mentioned previously, one at a time. A problem we saw with the original implementation was that the random forest training commented out included an option for down-sampling, whereas the other logistic regression models didn't have this option specified. This has the potential to yield unfair comparisons, hence we removed down-sampling. For dealing with class-imbalance, we will train models with our SMOTEd dataset later and test them on unseen, original (not artifically generated by SMOTE) data.

# Model Training
We will first train the 4 specifications mentioned in the paper with the ```data_train``` dataset, which was acquired by split (based on year) from the ```data_HS_imputed``` dataset provided by the authors. Then, finally, we will train the author specification (88 variables) with the ```data_train_balanced``` dataset. For each of the 5 versions of training, we will include a i) logistic regression model (LR) with uncorrected logits, ii) a logistic regression model (LR) with penalized logits using Firth's method, and iii) a random forest model (RF). All of the models use the cross-validation training control, ```tc```, implemented in the above code block.

1) Fearon and Laitin specification (2003) consisting of 11 variables
```{r LR-vs-RF-on-FL-specification}
set.seed(666) ## the most metal seed for CV

# Fearon and Laitin LR Model (2003) Uncorrected
model_FL_uncorrected <- train(FL_spec,
                              metric="ROC", method="glm", family="binomial", 
                              trControl=tc, data=data_train)

#summary(model_FL_uncorrected) ## provides coefficients & traditional R model output
#model_FL_uncorrected ## provides CV summary stats
#confusionMatrix(model_FL_uncorrected, norm="average") ## confusion matrix for predicted classes

# Fearon and Laitin LR Model (2003) Penalized
model_FL_penalized <- train(FL_spec,
                            metric="ROC", method="plr", # Firth's penalized LR
                            trControl=tc, data=data_train)
#summary(model_FL_penalized)
#model_FL_penalized
#confusionMatrix(model_FL_penalized, norm="average")

# Random Forest Model on Fearon and Laitin (2003) specification
model_FL_RF <- train(FL_spec,
                     metric="ROC", method="rf",
                     trControl=tc, data=data_train)
#summary(model_FL_RF)
#model_FL_RF
#confusionMatrix(model_FL_RF, norm="average")
```

2) Collier and Hoeffler specification (2004) consisting of 12 variables
```{r LR-vs-RF-on-CH-specification}
set.seed(666) ## the most metal seed for CV
# Collier and Hoeffler LR Model (2004) Uncorrected
model_CH_uncorrected <- train(CH_spec,
                              metric="ROC", method="glm", family="binomial", 
                              trControl=tc, data=data_train)

#summary(model_CH_uncorrected) ## provides coefficients & traditional R model output
#model_CH_uncorrected ## provides CV summary stats
#confusionMatrix(model_CH_uncorrected, norm="average") ## confusion matrix for predicted classes

# Collier and Hoeffler LR Model (2004) Penalized
model_CH_penalized <- train(CH_spec,
                            metric="ROC", method="plr", # Firth's penalized LR
                            trControl=tc, data=data_train)
#summary(model_CH_penalized)
#model_CH_penalized
#confusionMatrix(model_CH_penalized, norm="average")

# Random Forest Model on Collier and Hoeffler (2004) specification
model_CH_RF <- train(CH_spec,
                     metric="ROC", method="rf",
                     trControl=tc, data=data_train)
#summary(model_CH_RF)
#model_CH_RF
#confusionMatrix(model_CH_RF, norm="average")
```

3) Hegre and Sambanis specification (2006) consisting of 20 variables
```{r LR-vs-RF-on-HS-specification}
set.seed(666) ## the most metal seed for CV

# Hegre and Sambanis LR Model (2006) Uncorrected
model_HS_uncorrected <- train(HS_spec,
                              metric="ROC", method="glm", family="binomial", 
                              trControl=tc, data=data_train)

#summary(model_HS_uncorrected) ## provides coefficients & traditional R model output
#model_HS_uncorrected ## provides CV summary stats
#confusionMatrix(model_HS_uncorrected, norm="average") ## confusion matrix for predicted classes

# Hegre and Sambanis LR Model (2006) Penalized
model_HS_penalized <- train(HS_spec,
                            metric="ROC", method="plr", # Firth's penalized LR
                            trControl=tc, data=data_train)
#summary(model_HS_penalized)
#model_HS_penalized
#confusionMatrix(model_HS_penalized, norm="average")

# Random Forest Model on Hegre and Sambanis (2006) specification
model_HS_RF <- train(HS_spec,
                     metric="ROC", method="rf",
                     trControl=tc, data=data_train)
#summary(model_HS_RF)
#model_HS_RF
#confusionMatrix(model_HS_RF, norm="average")
```

4) The authors' specifications consisting of 88 variables, selected from Sambanis (2006) index
```{r LR-vs-RF-on-author-specification}
set.seed(666) ## the most metal seed for CV

# Author specification (2016) LR Model Uncorrected
model_AS_uncorrected <- train(author_spec,
                              metric="ROC", method="glm", family="binomial", 
                              trControl=tc, data=data_train)

#summary(model_AS_uncorrected) ## provides coefficients & traditional R model output
#model_AS_uncorrected ## provides CV summary stats
#confusionMatrix(model_AS_uncorrected, norm="average") ## confusion matrix for predicted classes

# Author specification (2016) LR Model Penalized (TAKING TOO LONG FOR SOME REASON...)
# model_AS_penalized <- train(author_spec,
#                             metric="ROC", method="plr", # Firth's penalized LR
#                             trControl=tc, data=data_train)
#summary(model_AS_penalized)
#model_AS_penalized
#confusionMatrix(model_AS_penalized, norm="average")

# Random Forest Model on author specification (2016) (TAKING TOO LONG FOR SOME REASON...)
# model_AS_RF <- train(author_spec,
#                      metric="ROC", method="rf",
#                      importance=T, ## Variable importance measures retained
#                      trControl=tc, data=data_train)
#summary(model_AS_RF)
#model_AS_RF
#confusionMatrix(model_AS_RF, norm="average")
```

5) The authors' specifications consisting of 88 variables, trained on ```data_train_balanced```, which was artificially generated from ```data_train``` using the SMOTE algorithm discussed previously.
```{r LR-vs-RF-on-author-specification-with-SMOTEd-dataset}
set.seed(666) ## the most metal seed for CV

# Author specification (2016) LR Model Uncorrected
model_AS_uncorrected_smoted <- train(author_spec,
                                     metric="ROC", method="glm", family="binomial", 
                                     trControl=tc, data=data_train_balanced)

#summary(model_AS_uncorrected_smoted) ## provides coefficients & traditional R model output
#model_AS_uncorrected_smoted ## provides CV summary stats
#confusionMatrix(model_AS_uncorrected_smoted, norm="average") ## confusion matrix for predicted classes

# Author specification (2016) LR Model Penalized
model_AS_penalized_smoted <- train(author_spec,
                                   metric="ROC", method="plr", # Firth's penalized LR
                                   trControl=tc, data=data_train_balanced)
#summary(model_AS_penalized_smoted)
#model_AS_penalized_smoted
#confusionMatrix(model_AS_penalized_smoted, norm="average")

# Random Forest Model on author specification (2016)
model_AS_RF_smoted <- train(author_spec,
                            metric="ROC", method="rf",
                            importance=T, ## Variable importance measures retained
                            trControl=tc, data=data_train_balanced)
#summary(model_AS_RF_smoted)
#model_AS_RF_smoted
#confusionMatrix(model_AS_RF_smoted, norm="average")
```

# Variable Importance Visualization for Random Forest Models
One advantage of the Random Forests algorithm is that it is easy to interpret, robust to outliers and noise, and allows the analyst to infer causal mechanisms. Although the paper has serious methodological flaws, its suggestion of Random Forests is well-grounded. The original R code by authors visualizes the variable importance for a Random Forest Model trained on the ```data_AM_imputed``` dataset, however we dropped that dataset as discussed before. It makes the most sense to visualize the variable importance on the 5 different random forest models we trained above, 4 on ```data_train``` and 1 on ```data_train_balanced```. By doing this, we can achieve: i) See if theories suggested by different parties on theoretical variables deciding a civil war onset agree with our findings or not, ii) Observe if any variables are deemed to be `unimportant` by the algorithm, and finally iii) Observe the difference of variable importance on random forests models trained on original training set and artifically created training set, and see how the SMOTE algorithms affects this measure.

Firstly, we will retrain random forest models with the same specs and data mentioned above, but with ```randomForests()``` class this same in order to plot importance of variables.
```{r retraining-RF-models}
FL_RF <- randomForest(FL_spec, importance=T, proximity=F, ntree=1000, 
                      confusion=T, err.rate=T, data=data_train)
CH_RF <- randomForest(CH_spec, importance=T, proximity=F, ntree=1000, 
                      confusion=T, err.rate=T, data=data_train)
HS_RF <- randomForest(HS_spec, importance=T, proximity=F, ntree=1000, 
                      confusion=T, err.rate=T, data=data_train)
AS_RF <- randomForest(author_spec, importance=T, proximity=F, ntree=1000, 
                      confusion=T, err.rate=T, data=data_train)
AS_RF_smoted <- randomForest(author_spec, importance=T, proximity=F, ntree=1000, 
                             confusion=T, err.rate=T, data=data_train_balanced)
```

Now, we can visualize the variable importances for each of the random forest models trained above.
```{r variable-importance-based-on-RF-models}
# Specify maximum number of variables (features) to be displayed
num_vars <- 20
varImpPlot(FL_RF, sort=T, type=2, 
           main="Fearon and Laitin (2003) Variables Importance on Training Accuracy", 
           n.var=num_vars)
varImpPlot(CH_RF, sort=T, type=2, 
           main="Collier and Hoeffler (2004) Variables Importance on Training Accuracy", 
           n.var=num_vars)
varImpPlot(HS_RF, sort=T, type=2, 
           main="Hegre and Sambanis (2006) Variables Importance on Training Accuracy", 
           n.var=num_vars)
varImpPlot(AS_RF, sort=T, type=2, 
           main="Author Specified (2016) Variables Importance on Training Accuracy", 
           n.var=num_vars)
varImpPlot(AS_RF_smoted, sort=T, type=2, 
           main="Author Specified (2016) Variables Importance on Training Accuracy w/ SMOTEd data", 
           n.var=num_vars)
```

# Model Testing
The paper includes a graph for comparing training AUC for uncorrected logistic regression models vs. the random forest model and a seperate graph for comparing the same metric for penalized logistic regression models vs. the random forest model. The problem with this approach is that, each of these model have different specifications and hence different numbers of features they are trained on. As discussed before, the random forest model even included downsampling which makes the comparison even less reliable due to possible overfitting. 

Here, we propose that we compare ROC curves and AUC scores for each of the specifications independently. With this method, we will be able to assess the performance of random forest algorithm in comparison to uncorrected & penalized logistic regression models in a more reliable way. We realize that the out-of-sample testing data contains a low number of examples, however this approach is still better than comparing training accuracies. Furthermore, this is due to the nature of this problem and the difficulty of data gathering associated with it. 

For each of the 3 models used for each of the 5 specifications and, we will predict with i) type="raw"
for number/class of predictions to be used in confusion matrices, and ii) type="prob" for class probabilities to be used with ROC curves and computation of AUC metric.
```{r testing-trained-models-on-out-of-sample-data}
# Testing/predictions for Fearon and Laitin specification (2003)
FL_uncorrected_pred_raw <- predict(model_FL_uncorrected, newdata=data_test, type="raw")
FL_uncorrected_pred_prob <- predict(model_FL_uncorrected, newdata=data_test, type="prob")
FL_penalized_pred_raw <- predict(model_FL_penalized, newdata=data_test, type="raw")
FL_penalized_pred_prob <- predict(model_FL_penalized, newdata=data_test, type="prob")
FL_RF_pred_raw <- predict(model_FL_RF, newdata=data_test, type="raw")
FL_RF_pred_prob <- predict(model_FL_RF, newdata=data_test, type="prob")

# Testing/predictions for Collier and Hoeffler specification (2004)
CH_uncorrected_pred_raw <- predict(model_CH_uncorrected, newdata=data_test, type="raw")
CH_uncorrected_pred_prob <- predict(model_CH_uncorrected, newdata=data_test, type="prob")
CH_penalized_pred_raw <- predict(model_CH_penalized, newdata=data_test, type="raw")
CH_penalized_pred_prob <- predict(model_CH_penalized, newdata=data_test, type="prob")
CH_RF_pred_raw <- predict(model_CH_RF, newdata=data_test, type="raw")
CH_RF_pred_prob <- predict(model_CH_RF, newdata=data_test, type="prob")

# Testing/predictions for Hegre and Sambanis specification (2006)
HS_uncorrected_pred_raw <- predict(model_HS_uncorrected, newdata=data_test, type="raw")
HS_uncorrected_pred_prob <- predict(model_HS_uncorrected, newdata=data_test, type="prob")
HS_penalized_pred_raw <- predict(model_HS_penalized, newdata=data_test, type="raw")
HS_penalized_pred_prob <- predict(model_HS_penalized, newdata=data_test, type="prob")
HS_RF_pred_raw <- predict(model_HS_RF, newdata=data_test, type="raw")
HS_RF_pred_prob <- predict(model_HS_RF, newdata=data_test, type="prob")

# Testing/predictions for author specification (2016)
AS_uncorrected_pred_raw <- predict(model_AS_uncorrected, newdata=data_test, type="raw")
AS_uncorrected_pred_prob <- predict(model_AS_uncorrected, newdata=data_test, type="prob")
# AS_penalized_pred_raw <- predict(model_AS_penalized, newdata=data_test, type="raw")
# AS_penalized_pred_prob <- predict(model_AS_penalized, newdata=data_test, type="prob")
# AS_RF_pred_raw <- predict(model_AS_RF, newdata=data_test, type="raw")
# AS_RF_pred_prob <- predict(model_AS_RF, newdata=data_test, type="prob")

# Testing/prediction for author specification (2016) trained on SMOTEd dataset
AS_uncorrected_smoted_pred_raw <- predict(model_AS_uncorrected_smoted, newdata=data_test, type="raw")
AS_uncorrected_smoted_pred_prob <- predict(model_AS_uncorrected_smoted, newdata=data_test, type="prob")
AS_penalized_smoted_pred_raw <- predict(model_AS_penalized_smoted, newdata=data_test, type="raw")
AS_penalized_smoted_pred_prob <- predict(model_AS_penalized_smoted, newdata=data_test, type="prob")
AS_RF_smoted_pred_raw <- predict(model_AS_RF_smoted, newdata=data_test, type="raw")
AS_RF_smoted_pred_prob <- predict(model_AS_RF_smoted, newdata=data_test, type="prob")
```

```{r confusion-matrices}
# Confusion matrices for prediction with Fearon and Laitin specification (2003)
table(pred=FL_uncorrected_pred_raw, obs=data_test$warstds)
table(pred=FL_penalized_pred_raw, obs=data_test$warstds)
table(pred=FL_RF_pred_raw, obs=data_test$warstds)

# Confusion matrices for prediction with Collier and Hoeffler specification (2004)
table(pred=CH_uncorrected_pred_raw, obs=data_test$warstds)
table(pred=CH_penalized_pred_raw, obs=data_test$warstds)
table(pred=CH_RF_pred_raw, obs=data_test$warstds)

# Confusion matrices for prediction with Hegre and Sambanis specification (2006)
table(pred=HS_uncorrected_pred_raw, obs=data_test$warstds)
table(pred=HS_penalized_pred_raw, obs=data_test$warstds)
table(pred=HS_RF_pred_raw, obs=data_test$warstds)

# Confusion matrices for prediction with author specification (2016)
table(pred=AS_uncorrected_pred_raw, obs=data_test$warstds)
# table(pred=AS_penalized_pred_raw, obs=data_test$warstds)
# table(pred=AS_RF_pred_raw, obs=data_test$warstds)

# Confusion matrices for prediction with author specification (2016) trained on SMOTEd dataset
table(pred=AS_uncorrected_smoted_pred_raw, obs=data_test$warstds)
table(pred=AS_penalized_smoted_pred_raw, obs=data_test$warstds)
table(pred=AS_RF_smoted_pred_raw, obs=data_test$warstds)
```

```{r seperation-plots}

```


# Dependencies Summary
The following is a list of all packages used to generate these results.
```{r}
sessionInfo()
```

# References
* https://stackoverflow.com/questions/11225343/how-to-create-a-world-map-in-r-with-specific-countries-filled-in
* https://stackoverflow.com/questions/20624698/fixing-set-seed-for-an-entire-session

