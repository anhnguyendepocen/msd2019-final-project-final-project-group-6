---
title: "MSD 2019 Final Project"
subtitle: "A replication and extension of Comparing Random Forest with Logistic Regression for Predicting Class-Imbalanced Civil War Onset Data by David Muchlinski, David Siroky, et. al., October 22, 2015"
author: "Your Names (your unis)"
date: '`r Sys.time()`'
output:
  pdf_document:
    toc: yes
    toc_depth: 3
---

```{r setup, include=FALSE}
# Setting up dependencies
library(randomForest)
library(caret)
library(ROCR)
library(pROC)
library(stepPlr)
library(doMC)
library(tidyverse)
library(foreign) # for reading Stata .dta files
library(maptools) # for displaying countries on world map
library(DMwR) # for up-sampling with SMOTE

theme_set(theme_bw())

knitr::opts_chunk$set(echo = TRUE)

registerDoMC(cores=7) ## distributing workload over multiple cores for faster computaiton
```

# Introduction

## Motivation
Prediction is at the heart of many machine learning and data science applications, and its importance is amplified in the context of political science. Especially for the case of civil war onset, robust models that can make correct predictions has the potential to save millions of lives and guide political agendas for the years to come.

## Paper Description
Comparing Random Forest with Logistic Regression for Predicting Class-Imbalanced Civil War Onset Data by David Muchlinski, David Siroky, et. al. is an explaratory research paper that, among several other things, makes the argument that most well-known logistic models acquire relatively lower predictive powers for civil war onsets, and shows that a custom random forest model achieves much higer prediction accuricies.

## Replication Description
The replication for the paper mentioned will consist of two main parts, which will go parallel to each other as represented in this R Notebook. The first part will deal with extracting code snippets from the original R code (found in ```original_code/```), and questioning the reasonability and correctness of the methods and code semantics used, as well as an effort to see if results can be completely replicated. The second part will deal with suggesting improvements, modifications, and eventually corrections to the original R code, where we will try to report summaries of each model using fair metrics and correct methodologies. 

Later, a third part will deal with suggesting new models...

# Data Exploration
```{r data-reading-and-exploration}
# HS_original: Civil War Data by Hegre and Sambanis (2006), the original version
data_HS_original <- read.dta(file="data/Sambanis (Aug 06).dta")

# HS_cleaned: Civil War Data by Hegre and Sambanis (2006), NAs eliminated version
data_HS_cleaned <- na.omit(data_HS_original)

# HS_imputed: Civil War Data by Hegre and Sambanis (2006), imputed by authors
data_HS_imputed <- read.csv(file="data/SambnisImp.csv") ## data for prediction

# AM_imputed: Amelia dataset imputed by authors
data_AM_imputed <- read.csv(file="data/Amelia.Imp3.csv") ## data for causal machanisms

# AF_imputed: Africa dataset imputed by authors
data_AF_imputed <- read.csv(file="data/AfricaImp.csv")

data_presentation <- matrix(c(ncol(data_HS_original), sum(is.na(data_HS_original)),
                              nrow(data_HS_original),
                              ncol(data_HS_cleaned), sum(is.na(data_HS_cleaned)),
                              nrow(data_HS_cleaned),
                              ncol(data_HS_imputed), sum(is.na(data_HS_imputed)),
                              nrow(data_HS_imputed),
                              ncol(data_AM_imputed), sum(is.na(data_AM_imputed)),
                              nrow(data_AM_imputed),
                              ncol(data_AF_imputed), sum(is.na(data_AF_imputed)),
                              nrow(data_AF_imputed)), ncol=5)
colnames(data_presentation) <- c('HS_original', 'HS_cleaned', 
                                 'HS_imputed', 'AM_imputed', 'AF_imputed')
rownames(data_presentation) <- c('No. features', 'No. empty cells', 'No. examples')
as.data.frame(data_presentation)

# Check intersection and difference of features on two datasets
# setdiff(colnames(data_HS_imputed), colnames(data_HS_original))
# length(intersect(colnames(data_HS_imputed), colnames(data_HS_original)))
# intersect(colnames(data_HS_imputed), colnames(data_AF_imputed))

# Function for converting a specified dependent variable into factor levels
Y_factor <- function(dataset_column) {
  return(factor(dataset_column, levels=c(0,1), labels=c("peace", "war")))
}

# Convert 'warstds' column into 'peace' or 'war' (initially marked 0 or 1)
data_HS_original$warstds <- Y_factor(data_HS_original$warstds)
data_HS_imputed$warstds <- Y_factor(data_HS_imputed$warstds)
data_AM_imputed$warstds <- Y_factor(data_AM_imputed$warstds)
data_AF_imputed$warstds <- Y_factor(data_AF_imputed$warstds)

# Visualize countries with civil war for each dataset
data(wrld_simpl)
HS_original_countries <- wrld_simpl@data$NAME %in% 
  data_HS_original[data_HS_original$warstds=='war',]$country
AM_imputed_countries <- wrld_simpl@data$NAME %in% 
  data_AM_imputed[data_AM_imputed$warstds=='war',]$country

plot(wrld_simpl, col=c(gray(.80),"red")[HS_original_countries+1], 
     main='Original Hegre and Sambanis (2006)')
plot(wrld_simpl, col=c(gray(.80),"red")[AM_imputed_countries+1],
     main='Imputed Amelia Dataset')
```

There are a few questions that remained unanswered after replicating the data reading and processing done by authors. 

The reported presentation data frame above shows that the original Hegre and Sambanis (2006) dataset, denoted by ```HS_original```, has 9691 examples, 284 features, and 979981 cells containing missing (NA) values. The dataset that is constructed by ourselves when these cells were omitted, ```HS_cleaned```, shows that every single row of the original dataset contains some missing values. Obviously, this dataset is dropped from now on as it doesn't contain any entries. ```HS_imputed```, the dataset imputed by authors on the other hand, has 7140 examples, 286 features, and 0 cells containing missing (NA) values. It is unclear and unmentioned how and why the authors have imputed this dataset, filled all cells with missing values, and deleted ~2500 examples from the original dataset. The second dataset imputed by authors, denoted ```AM_imputed```, has 7141 examples, 53 features, and 778 cells containing missing (NA) values. Lastly, the third dataset imputed by authors, denoted ```AF_imputed```, has 737 examples, 11 features, and 0 cells containing missing (NA) values. This dataset is also dropped, because i) no country information exists whatsoever on examples, and more importantly ii) none of the features (columns) in this dataset match with any of the other features in the other datasets (except the dependent variable). 

It is also unclear why the paper needs two different imputed datasets. The ```AM_imputed``` dataset is supposed to be the smaller dataset where features theorized to be most relevant to the onset of civil war are imputed. Although the number of features decrease as claimed, 778 cells with missing (NA) values reappear in this dataset, and the number of examples increase by 1 without any explanation. For comparison and metrics reporting purposes anyway, it is usually a better idea to select a singular dataset and train & test models on this same dataset through a reasonable splitting of data. On the contrary, the authors have only used in-sample metric to report the accuracies of the models, hence the reported accuracies don't really say much. In order to assess performance, we have to test our models for out-of-sample data.

# Model Specifications

To keep track of model specifications which use different numbers and types of features based on either theory or computations, we propose that we explicitly define features and hence accompanying formulas to be used by models later in this next code block. The 4 specifications the paper have covered are:

1) Their own specifications consisting of 88 variables, selected from Sambanis (2006) index
2) Fearon and Laitin specification (2003) consisting of 11 variables
3) Collier and Hoeffler specification (2004) consisting of 12 variables
4) Hegre and Sambanis specification (2006) consisting of 20 variables

```{r data-feature-selection}
# Function to generate a formula given dependent variable label and features
# Ex: get_model_formula('height', c('age', 'weight')) : height ~ age + weight
get_model_formula <- function(label, feature_vector) {
  formula_string <- ""
  for (feature in feature_vector) {
    formula_string <- paste(formula_string, feature, "+")
  }
  formula_string <- substring(formula_string, 1, nchar(formula_string)-1)
  return(as.formula(paste(paste(label, "~"), formula_string)))
}

# Specify the dependent variable that will be predicted in all models
y_var <- "warstds"

# The 88 variables selected by authors from Sambanis (2006) Appendix as spec of their RF model
author_vars <- c("ager", "agexp", "anoc", "army85", "autch98", "auto4",
                 "autonomy", "avgnabo", "centpol3", "coldwar", "decade1", "decade2",
                 "decade3", "decade4", "dem", "dem4", "demch98", "dlang", "drel",
                 "durable", "ef", "ef2", "ehet", "elfo", "elfo2", "etdo4590",
                 "expgdp", "exrec", "fedpol3", "fuelexp", "gdpgrowth", "geo1", "geo2",
                 "geo34", "geo57", "geo69", "geo8", "illiteracy", "incumb", "infant",
                 "inst", "inst3", "life", "lmtnest", "ln_gdpen", "lpopns", "major", "manuexp",
                 "milper", "mirps0", "mirps1", "mirps2", "mirps3", "nat_war", "ncontig",
                 "nmgdp", "nmdp4_alt", "numlang", "nwstate", "oil", "p4mchg",
                 "parcomp", "parreg", "part", "partfree", "plural", "plurrel",
                 "pol4", "pol4m", "pol4sq", "polch98", "polcomp", "popdense",
                 "presi", "pri", "proxregc", "ptime", "reg", "regd4_alt", "relfrac", 
                 "seceduc", "second", "semipol3", "sip2", "sxpnew", "sxpsq", "tnatwar",
                 "trade", "warhist", "xconst")
author_spec <- get_model_formula(y_var, author_vars)

# The 11 variables selected by Fearon and Laitin (2003) as spec of their LR model
FL_vars <- c("warhist", "ln_gdpen", "lpopns", "lmtnest", "ncontig",
             "oil", "nwstate", "inst3", "pol4", "ef", "relfrac")
FL_spec <- get_model_formula(y_var, FL_vars)

# The 12 variables selected by Collier and Hoeffler (2004) as spec of their LR model
CH_vars <- c("sxpnew", "sxpsq", "ln_gdpen", "gdpgrowth", "warhist", "lmtnest", 
             "ef", "popdense", "lpopns", "coldwar", "seceduc", "ptime")
CH_spec <- get_model_formula(y_var, CH_vars)

# The 20 variables selected by Hegre and Sambanis (2006) as spec of their LR model
HS_vars <- c("lpopns", "ln_gdpen", "inst3", "parreg", "geo34", "proxregc", "gdpgrowth",
             "anoc", "partfree", "nat_war", "lmtnest", "decade1", "pol4sq", "nwstate", 
             "regd4_alt", "etdo4590", "milper", "geo1", "tnatwar", "presi")
HS_spec <- get_model_formula(y_var, HS_vars)
```

```{r class-imbalance-exploration}
# Explore class imbalance
table(data_HS_original$warstds)
table(data_HS_imputed$warstds)
table(data_AM_imputed$warstds)

# SMOTE for artificially creating a more balanced dataset
data_HS_balanced <- SMOTE(warstds ~ ., data_HS_imputed, perc.over = 600, perc.under=100)
table(data_HS_balanced$warstds)
```

With the first three reported tables, it seems that authors have either deleted or modified examples from the ```HS_original``` dataset where the ```warstds``` variable was NA when they were preparing their imputed dataset ```HS_imputed```. Hence, they increased examples of peace by 800 examples in their imputation. In such a class-imbalanced data (as displayed by tables above), one would imagine i) down-sampling from the majority class or ii) up-sampling from the minority class would be the reasonable action, rather than increasing the number of examples of the majority class. Hence, we will implement these two techniques, i) and ii), and see if they yield better results.

SMOTE (Synthetic Minority Over-sampling Technique) is designed for problems when one class dominates the other, which usually happens in rare-event occurrences. We can easily make the argument that it is thus appropraite for the civil war onset data at hand. The general idea of this method is to artificially generate new examples of the minority class using the nearest neighbors of these cases. Furthermore, the majority class examples are also under-sampled, leading to a more balanced dataset. (RDocumentation) Hence, it is a combination of i) and ii).

Below is the setup to be run before training. We are not changing anything here. One thing to look for is that we will set the seed for each caret training function call, so that all of the results presented here are replicable. The original code by the authors only set the seed once, which unfortunately makes their results unreplicable.
```{r setup-before-training}
set.seed(666) ## the most metal seed for CV

## This method of data slicing - or CV - will be used for all logit models
tc <- trainControl(method="cv", 
                   number=10, ## creates CV folds - 10 for this data
                   summaryFunction=twoClassSummary, ## provides ROC stats in call to model
                   classProb=T)

# Since we are done with visualizations and exploration, refactor the dependent variable
data_HS_imputed$warstds <- as.factor(data_HS_imputed$warstds)
data_AM_imputed$warstds <- as.factor(data_AM_imputed$warstds)
data_HS_balanced$warstds <- as.factor(data_HS_balanced$warstds)
```

Now that everything is set up, we will train and test logistic regression and random forest models (* only in-sample as of now) using the 4 specifications mentioned previously, one at a time. A problem we saw with the original implementation was that the random forest training commented out included an option for down-sampling, whereas the other logistic regression models didn't have this option specified. This has the potential to yield unfair comparisons, hence we removed down-sampling. For dealing with class-imbalance, we will train models with our SMOTEd dataset later and test them on unseen, original (not artifically generated by SMOTE) data.
```{r LR-vs-RF-on-FL-specification}
set.seed(666) ## the most metal seed for CV

# Fearon and Laitin LR Model (2003) Uncorrected
model_FL_uncorrected <- train(FL_spec,
                              metric="ROC", method="glm", family="binomial", 
                              trControl=tc, data=data_HS_imputed)

#summary(model_FL_uncorrected) ## provides coefficients & traditional R model output
#model_FL_uncorrected ## provides CV summary stats
#confusionMatrix(model_FL_uncorrected, norm="average") ## confusion matrix for predicted classes

# Fearon and Laitin LR Model (2003) Penalized
model_FL_penalized <- train(FL_spec,
                            metric="ROC", method="plr", # Firth's penalized LR
                            trControl=tc, data=data_HS_imputed)
#summary(model_FL_penalized)
#model_FL_penalized
#confusionMatrix(model_FL_penalized, norm="average")

# Random Forest Model on Fearon and Laitin (2003) specification
model_FL_RF <- train(FL_spec,
                     metric="ROC", method="rf",
                     trControl=tc, data=data_HS_imputed)
#summary(model_FL_RF)
#model_FL_RF
#confusionMatrix(model_FL_RF, norm="average")
```


```{r LR-vs-RF-on-CH-specification}
set.seed(666) ## the most metal seed for CV

# Collier and Hoeffler LR Model (2004) Uncorrected
model_CH_uncorrected <- train(CH_spec,
                              metric="ROC", method="glm", family="binomial", 
                              trControl=tc, data=data_HS_imputed)

#summary(model_CH_uncorrected) ## provides coefficients & traditional R model output
#model_CH_uncorrected ## provides CV summary stats
#confusionMatrix(model_CH_uncorrected, norm="average") ## confusion matrix for predicted classes

# Collier and Hoeffler LR Model (2004) Penalized
model_CH_penalized <- train(CH_spec,
                            metric="ROC", method="plr", # Firth's penalized LR
                            trControl=tc, data=data_HS_imputed)
#summary(model_CH_penalized)
#model_CH_penalized
#confusionMatrix(model_CH_penalized, norm="average")

# Random Forest Model on Collier and Hoeffler (2004) specification
model_CH_RF <- train(CH_spec,
                     metric="ROC", method="rf",
                     trControl=tc, data=data_HS_imputed)
#summary(model_CH_RF)
#model_CH_RF
#confusionMatrix(model_CH_RF, norm="average")
```

```{r LR-vs-RF-on-HS-specification}
set.seed(666) ## the most metal seed for CV

# Hegre and Sambanis LR Model (2006) Uncorrected
model_HS_uncorrected <- train(HS_spec,
                              metric="ROC", method="glm", family="binomial", 
                              trControl=tc, data=data_HS_imputed)

#summary(model_HS_uncorrected) ## provides coefficients & traditional R model output
#model_HS_uncorrected ## provides CV summary stats
#confusionMatrix(model_HS_uncorrected, norm="average") ## confusion matrix for predicted classes

# Hegre and Sambanis LR Model (2006) Penalized
model_HS_penalized <- train(HS_spec,
                            metric="ROC", method="plr", # Firth's penalized LR
                            trControl=tc, data=data_HS_imputed)
#summary(model_HS_penalized)
#model_HS_penalized
#confusionMatrix(model_HS_penalized, norm="average")

# Random Forest Model on Hegre and Sambanis (2006) specification
model_HS_RF <- train(HS_spec,
                     metric="ROC", method="rf",
                     trControl=tc, data=data_HS_imputed)
#summary(model_HS_RF)
#model_HS_RF
#confusionMatrix(model_HS_RF, norm="average")
```

# Dependencies Summary
The following is a list of all packages used to generate these results.
```{r}
sessionInfo()
```

# References
* https://stackoverflow.com/questions/11225343/how-to-create-a-world-map-in-r-with-specific-countries-filled-in
* https://stackoverflow.com/questions/20624698/fixing-set-seed-for-an-entire-session

