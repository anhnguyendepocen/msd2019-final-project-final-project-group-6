---
title: "MSD 2019 Final Project"
subtitle: "A replication and extension of Comparing Random Forest with Logistic Regression for Predicting Class-Imbalanced Civil War Onset Data by David Muchlinski, David Siroky, et. al., October 22, 2015"
author: "Your Names (your unis)"
date: '`r Sys.time()`'
output:
  pdf_document:
    toc: yes
    toc_depth: 3
---

```{r setup, include=FALSE}
# Setting up dependencies
library(randomForest)
library(caret)
library(ROCR)
library(pROC)
library(stepPlr)
library(doMC)
library(tidyverse)
library(foreign) # for reading Stata .dta files
library(maptools) # for displaying countries on world map

theme_set(theme_bw())

knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

## Motivation
Prediction is at the heart of many machine learning and data science applications, and its importance is amplified in the context of political science. Especially for the case of civil war onset, robust models that can make correct predictions has the potential to save millions of lives and guide political agendas for the years to come.

## Paper Description
Comparing Random Forest with Logistic Regression for Predicting Class-Imbalanced Civil War Onset Data by David Muchlinski, David Siroky, et. al. is an explaratory research paper that, among several other things, makes the argument that most well-known logistic models acquire relatively lower predictive powers for civil war onsets, and shows that a custom random forest model achieves much higer prediction accuricies.

## Replication Description
The replication for the paper mentioned will consist of two main parts, which will go parallel to each other as represented in this R Notebook. The first part will deal with extracting code snippets from the original R code (found in ```original_code/```), and questioning the reasonability and correctness of the methods and code semantics used, as well as an effort to see if results can be completely replicated. The second part will deal with suggesting improvements, modifications, and eventually corrections to the original R code, where we will try to report summaries of each model using fair metrics and correct methodologies. 

Later, a third part will deal with suggesting new models...

# Data Exploration

```{r data-reading-and-exploration}
# HS_original: Civil War Data by Hegre and Sambanis (2006), the original version
data_HS_original <- read.dta(file="data/Sambanis (Aug 06).dta")

# HS_cleaned: Civil War Data by Hegre and Sambanis (2006), NAs eliminated version
data_HS_cleaned <- na.omit(data_HS_original)

# HS_imputed: Civil War Data by Hegre and Sambanis (2006), imputed by authors
data_HS_imputed <- read.csv(file="data/SambnisImp.csv") ## data for prediction

# AM_imputed: Amelia dataset imputed by authors
data_AM_imputed <- read.csv(file="data/Amelia.Imp3.csv") ## data for causal machanisms
# unique(data_AM_imputed$country) # check countries

data_presentation <- matrix(c(ncol(data_HS_original), sum(is.na(data_HS_original)),
                              nrow(data_HS_original),
                              ncol(data_HS_cleaned), sum(is.na(data_HS_cleaned)),
                              nrow(data_HS_cleaned),
                              ncol(data_HS_imputed), sum(is.na(data_HS_imputed)),
                              nrow(data_HS_imputed),
                              ncol(data_AM_imputed), sum(is.na(data_AM_imputed)),
                              nrow(data_AM_imputed)), ncol=4)
colnames(data_presentation) <- c('HS_original', 'HS_cleaned', 'HS_imputed', 'AM_imputed')
rownames(data_presentation) <- c('No. features', 'No. empty cells', 'No. examples')
as.data.frame(data_presentation)

# Check intersection and difference of features on two datasets
# setdiff(colnames(data_HS_imputed), colnames(data_HS_original))
# length(intersect(colnames(data_HS_imputed), colnames(data_HS_original)))

# Function for converting a specified dependent variable into factor levels
Y_factor <- function(dataset_column) {
  return(factor(dataset_column, levels=c(0,1), labels=c("peace", "war")))
}

# Convert 'warstds' column into 'peace' or 'war' (initially marked 0 or 1)
data_HS_original$warstds <- Y_factor(data_HS_original$warstds)
data_HS_imputed$warstds <- Y_factor(data_HS_imputed$warstds)
data_AM_imputed$warstds <- Y_factor(data_AM_imputed$warstds)

# Visualize countries with civil war for each dataset
data(wrld_simpl)
HS_original_countries = wrld_simpl@data$NAME %in% 
  data_HS_original[data_HS_original$warstds=='war',]$country
AM_imputed_countries = wrld_simpl@data$NAME %in% 
  data_AM_imputed[data_AM_imputed$warstds=='war',]$country

plot(wrld_simpl, col = c(gray(.80), "red")[HS_original_countries+1])
plot(wrld_simpl, col = c(gray(.80), "blue")[AM_imputed_countries+1])

```

There are a few questions that remained unanswered after replicating the data reading and processing done by authors. 

The reported presentation data frame above shows that the original Hegre and Sambanis (2006) dataset, denoted by ```HS_original```, has 9691 examples, 284 features, and 979981 cells containing missing (NA) values. The dataset that is constructed by ourselves when these cells were omitted, ```HS_cleaned```, shows that every single row of the original dataset contains some missing values. ```HS_imputed```, the dataset imputed by authors on the other hand, has 7140 examples, 286 features, and 0 cells containing missing (NA) values. It is unclear and unmentioned how and why the authors have imputed this dataset, filled all cells with missing values, and deleted ~2500 examples from the original dataset. Lastly, the second dataset imputed by authors, denoted ```AM_imputed```, has 7141 examples, 53 features, and 778 cells containing missing (NA) values.

It is also unclear why the paper needs two different imputed datasets. The ```AM_imputed``` dataset is supposed to be the smaller dataset where features theorized to be most relevant to the onset of civil war are imputed. Although the number of features decrease as claimed, 778 cells with missing (NA) values reappear in this dataset, and the number of examples increase by 1 without any explanation. For comparison and metrics reporting purposes anyway, it is usually a better idea to select a singular dataset and train & test models on this same dataset through a reasonable splitting of data. 

```{r data-processing}
# The 88 variables selected by authors from Sambanis (2006) Appendix
vars <- c("warstds", "ager", "agexp", "anoc", "army85", "autch98", "auto4",
          "autonomy", "avgnabo", "centpol3", "coldwar", "decade1", "decade2",
          "decade3", "decade4", "dem", "dem4", "demch98", "dlang", "drel",
          "durable", "ef", "ef2", "ehet", "elfo", "elfo2", "etdo4590",
          "expgdp", "exrec", "fedpol3", "fuelexp", "gdpgrowth", "geo1", "geo2",
          "geo34", "geo57", "geo69", "geo8", "illiteracy", "incumb", "infant",
          "inst", "inst3", "life", "lmtnest", "ln_gdpen", "lpopns", "major", "manuexp", 
          "milper", "mirps0", "mirps1", "mirps2", "mirps3", "nat_war", "ncontig",
          "nmgdp", "nmdp4_alt", "numlang", "nwstate", "oil", "p4mchg",
          "parcomp", "parreg", "part", "partfree", "plural", "plurrel",
          "pol4", "pol4m", "pol4sq", "polch98", "polcomp", "popdense",
          "presi", "pri", "proxregc", "ptime", "reg", "regd4_alt", "relfrac", "seceduc",
          "second", "semipol3", "sip2", "sxpnew", "sxpsq", "tnatwar", "trade",
          "warhist", "xconst")
```

```{r class-imbalance-exploration}
table(data_HS_original$warstds)
table(data_HS_imputed$warstds)
table(data_AM_imputed$warstds)
```

With these reported tables, it seems that authors have either deleted or modified examples from the ```HS_original``` dataset where the ```warstds``` variable was NA when they were preparing their imputed dataset ```HS_imputed```. Hence, they increased examples of peace by 800 examples in their imputation. In such a class-imbalanced data (as displayed by tables above), one would imagine i) down-sampling from the majority class or ii) up-sampling from the minority class would be the reasonable action, rather than increasing the number of examples of the majority class. Hence, we will implement these two techniques, i) and ii), and see if they yield better results.


The following is a list of all packages used to generate these results. (Leave at very end of file.)

```{r}
sessionInfo()
```